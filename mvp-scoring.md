# MVP: Scoring System Specification

## Overview

Your interview platform uses a **dual-layer scoring system** combining automated test results with your custom manual assessment parameters.

```
FINAL SCORE = (Auto Score Ã— 0.4) + (Manual Score Ã— 0.6)
Range: 0-100 (no decimal rounding)
No automatic recommendation - hiring team decides
```

---

## Auto Score (40% Weight)

### What It Is
Objective score based on test execution in the sandbox.

### Calculation
```
Tests Passed: 7/7
Auto Score: 87/100
```

### How It Works

1. **Candidate submits solution**
2. **Backend runs hidden tests** (predefined for each question)
3. **Tests execute in Docker sandbox** (10-second timeout, 128MB memory)
4. **Results calculated:**
   - Tests passed: 7
   - Total tests: 7
   - Score: (7/7) Ã— 100 = 100... but adjusted for difficulty
   - Final: 87/100 (account for code quality, efficiency)

### Example Test Results
```json
{
  "test_passed": 7,
  "test_total": 7,
  "test_details": [
    {"name": "basic_case", "passed": true},
    {"name": "empty_input", "passed": true},
    {"name": "null_case", "passed": true},
    {"name": "large_input", "passed": true},
    {"name": "edge_case_1", "passed": true},
    {"name": "edge_case_2", "passed": true},
    {"name": "performance", "passed": true}
  ],
  "auto_score_original": 87
}
```

### Manual Adjustment (Optional)

Interviewer can **manually adjust** the auto score with a required reason.

**When to adjust:**
- Different quality of test vs actual solution
- Edge case handled but not tested
- Candidate showed understanding despite test failure
- Timing/environment issue caused failure
- Tests were too strict/lenient

**Process:**
```
â˜ Manually adjust score
   [87] / 100
   Why? [Candidate explained solution well 
         even though one edge case failed]
```

**Database Storage:**
```sql
auto_score_original INT,         -- 87 (from tests)
auto_score_adjusted INT,         -- 75 (if interviewer changed)
auto_score_adjusted_reason TEXT, -- "Explanation"
```

---

## Manual Assessment (60% Weight)

### Your 4 Custom Parameters

```
Parameter                      Points    Weight    Total %
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. COMMUNICATION               0-15      22%       13.2%
2. ALGORITHMIC THINKING        0-20      28%       16.8%
3. PROBLEM SOLVING             0-20      28%       16.8%
4. AI COLLABORATION            0-15      22%       13.2%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL MANUAL:                  0-70      100%      60%
```

### Parameter Definitions

#### 1. COMMUNICATION (0-15 points)
**Focus:** Can they explain their thinking clearly?

**What to assess:**
- Can explain solution step-by-step
- Uses correct technical terminology
- Can trace through examples
- Responsive to clarifying questions (not defensive)

**Questions to ask:**
- "Walk me through your solution. How does it work?"
- "Can you explain this part in more detail?"
- "Let me give you an example - can you trace through?"

**Red Flags (0-5 points):**
- "I don't understand this part of the code"
- "The AI wrote this, so..."
- Can't trace through own examples
- Gets defensive when questioned
- Uses vague explanations

**Green Flags (10-15 points):**
- Clear step-by-step explanation
- Uses correct technical terms naturally
- Adjusts explanation for different audience levels
- Responsive to questions, asks clarifying questions back
- Confident but humble

**Scoring Guide:**
- **0-5:** Can't explain solution, very vague
- **6-10:** Basic explanation, some confusion
- **11-13:** Good explanation, minor gaps
- **14-15:** Clear, confident, thorough explanation

---

#### 2. ALGORITHMIC THINKING (0-20 points)
**Focus:** Do they think about edge cases and alternatives?

**What to assess:**
- Identifies edge cases proactively (or when asked)
- Considers alternative approaches
- Discusses time/space tradeoffs
- Tests boundary conditions

**Questions to ask:**
- "What edge cases did you consider for this problem?"
- "Are there alternative approaches to solve this?"
- "What are the time and space complexity tradeoffs?"
- "Did you test with boundary conditions (empty, null, max values)?"

**Red Flags (0-7 points):**
- "I didn't think about edge cases"
- "There's only one way to solve this"
- "I just used the first approach that worked"
- Gets defensive about missing edge cases
- Can't identify any potential issues
- No discussion of complexity

**Green Flags (15-20 points):**
- "For empty input, we need to handle..."
- "I could also solve this with... but it's slower"
- "Time complexity is O(n), space is O(1)"
- "I tested with [specific edge cases]"
- Voluntarily mentions alternative approaches

**Scoring Guide:**
- **0-7:** Doesn't think about edges, no alternatives
- **8-12:** Mentions edges when asked, one alternative
- **13-16:** Proactively identifies edges, multiple alternatives
- **17-20:** Comprehensive edge case handling, thorough alternative analysis

---

#### 3. PROBLEM SOLVING (0-20 points)
**Focus:** Is their code clean and do they debug systematically?

**What to assess:**
- Code is clean, readable, well-structured
- Proper error handling
- Systematic debugging when issues arise
- Willing to refactor for clarity

**Code Quality Checklist:**
- â˜ Good variable/function naming (meaningful)
- â˜ Proper error handling (try/catch, null checks)
- â˜ No unnecessary complexity (KISS principle)
- â˜ Follows language conventions
- â˜ Well-structured functions (single responsibility)
- â˜ Readable (not cryptic one-liners)

**Questions to ask:**
- "Walk me through your code structure"
- "How did you debug when tests failed?"
- "Can you refactor this section to be clearer?"
- "How do you handle errors?"

**Red Flags (0-7 points):**
- "I just tried random things until it worked"
- Code is messy, hard to follow
- No error handling
- Single-letter variables everywhere
- Massive functions doing multiple things
- Gets defensive about code quality

**Green Flags (15-20 points):**
- Clean, readable code
- Comprehensive error handling
- Systematic debugging approach ("I added logs here...")
- Willing to refactor ("I could make this clearer by...")
- Well-commented where helpful
- Follows best practices for the language

**Scoring Guide:**
- **0-7:** Code is messy, no error handling
- **8-12:** Code works, some quality issues
- **13-16:** Clean code, good error handling
- **17-20:** Production-quality code, excellent practices

---

#### 4. AI COLLABORATION (0-15 points)
**Focus:** Do they use AI effectively? Understand it? Improve it?

**What to assess:**
- Used AI for implementation help
- Understands the AI-generated code
- Tests AI suggestions before using
- Reviews and improves upon AI code
- Can explain all code (including AI parts)

**Questions to ask:**
- "Did you use AI to help? Where?"
- "Can you explain the AI-generated parts?"
- "Did you test the AI code before using it?"
- "Did you improve or change anything the AI wrote?"
- "What issues did you find in the AI code?"

**Red Flags (0-5 points):**
- "I just copied the AI code as-is"
- "I didn't test because I trusted the AI"
- "I can't explain this part"
- "The AI wrote it, I just used it"
- Gets defensive when asked to explain
- Didn't review for bugs

**Green Flags (12-15 points):**
- "I tested each suggestion before using"
- "I reviewed the code and found a bug"
- "I refactored this part for clarity"
- "I questioned the approach and tried another way"
- Can explain all code thoroughly
- "I improved the error handling"

**Scoring Guide:**
- **0-5:** Copied AI code, didn't understand/test
- **6-9:** Used AI, tested minimally, mostly understood
- **10-12:** Good AI usage, tested thoroughly, minor improvements
- **13-15:** Excellent AI collaboration, thorough review, significant improvements

---

## Manual Score Calculation

### Formula

```
Manual Score = 
  (communication_score / 15 Ã— 0.22 Ã— 100) +
  (algorithmic_score / 20 Ã— 0.28 Ã— 100) +
  (problem_solving_score / 20 Ã— 0.28 Ã— 100) +
  (ai_collaboration_score / 15 Ã— 0.22 Ã— 100)
```

### Example Calculation

**Interviewer scores:**
- Communication: 11/15
- Algorithmic Thinking: 16/20
- Problem Solving: 16/20
- AI Collaboration: 12/15

**Calculation:**
```
Communication:     (11/15) Ã— 0.22 Ã— 100 = 16.13 pts
Algorithmic:       (16/20) Ã— 0.28 Ã— 100 = 22.40 pts
Problem Solving:   (16/20) Ã— 0.28 Ã— 100 = 22.40 pts
AI Collaboration:  (12/15) Ã— 0.22 Ã— 100 = 17.60 pts
                                           â”€â”€â”€â”€â”€â”€â”€â”€â”€
Manual Score:                              78/100
```

### Interpretation
- **0-40:** Significant concerns
- **41-60:** Borderline, needs improvement
- **61-75:** Good baseline
- **76-85:** Strong performance
- **86-100:** Excellent performance

---

## Final Score Calculation

### Formula

```
Final Score = (Auto Score Ã— 0.4) + (Manual Score Ã— 0.6)
```

### Example

```
Auto Score:        87 Ã— 0.4 = 34.8 points
Manual Score:      78 Ã— 0.6 = 46.8 points
                              â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final Score:                  81.6 â‰ˆ 82/100
```

### Interpretation

No automatic recommendation. Use final score + notes for decision:

- **0-50:** Not ready for role
- **51-70:** Concerning patterns, needs significant improvement
- **71-80:** Meets baseline expectations, decent candidate
- **81-90:** Strong candidate, ready to hire
- **91-100:** Exceptional candidate, standout performance

---

## Database Schema

### interview_sessions Table

```sql
CREATE TABLE interview_sessions (
    id BIGSERIAL PRIMARY KEY,
    interview_id BIGINT REFERENCES interviews(id) UNIQUE NOT NULL,
    
    -- Auto Score
    test_passed INT,
    test_total INT,
    auto_score_original INT CHECK (auto_score_original >= 0 AND auto_score_original <= 100),
    auto_score_adjusted INT CHECK (auto_score_adjusted >= 0 AND auto_score_adjusted <= 100),
    auto_score_adjusted_reason TEXT,
    
    -- Manual Scores (4 parameters)
    manual_score_communication INT CHECK (manual_score_communication >= 0 AND manual_score_communication <= 15),
    manual_score_algorithmic INT CHECK (manual_score_algorithmic >= 0 AND manual_score_algorithmic <= 20),
    manual_score_problem_solving INT CHECK (manual_score_problem_solving >= 0 AND manual_score_problem_solving <= 20),
    manual_score_ai_collaboration INT CHECK (manual_score_ai_collaboration >= 0 AND manual_score_ai_collaboration <= 15),
    
    -- Notes for each parameter
    notes_communication TEXT,
    notes_algorithmic TEXT,
    notes_problem_solving TEXT,
    notes_ai_collaboration TEXT,
    
    -- Custom observations
    custom_observations TEXT,
    
    -- Final score (calculated, stored for reference)
    final_score INT CHECK (final_score >= 0 AND final_score <= 100),
    
    -- Timestamps
    evaluation_submitted_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Indexes

```sql
CREATE INDEX idx_interview_sessions_interview ON interview_sessions(interview_id);
CREATE INDEX idx_interview_sessions_final_score ON interview_sessions(final_score);
CREATE INDEX idx_interview_sessions_submitted ON interview_sessions(evaluation_submitted_at);
```

---

## API Endpoints

### Submit Evaluation

```
POST /api/evaluations
Content-Type: application/json

{
  "interview_id": 123,
  
  "auto_score_adjusted": 82,
  "auto_score_adjusted_reason": "Candidate explained solution well despite test failure",
  
  "manual_score_communication": 11,
  "notes_communication": "Clear explanation, became defensive when questioned",
  
  "manual_score_algorithmic": 16,
  "notes_algorithmic": "Good edge case discussion, mentioned alternatives",
  
  "manual_score_problem_solving": 16,
  "notes_problem_solving": "Clean code, good error handling, debugged methodically",
  
  "manual_score_ai_collaboration": 12,
  "notes_ai_collaboration": "Tested thoroughly, caught a bug in AI code",
  
  "custom_observations": "Strong collaborative skills, good fit for mentorship program"
}

Response:
{
  "id": 456,
  "interview_id": 123,
  "final_score": 82,
  "evaluation_submitted_at": "2025-12-23T14:30:00Z"
}
```

### Get Evaluation

```
GET /api/evaluations/123

Response:
{
  "id": 456,
  "interview_id": 123,
  
  "auto_score_original": 87,
  "auto_score_adjusted": 82,
  "auto_score_adjusted_reason": "...",
  
  "manual_score_communication": 11,
  "notes_communication": "...",
  
  "manual_score_algorithmic": 16,
  "notes_algorithmic": "...",
  
  "manual_score_problem_solving": 16,
  "notes_problem_solving": "...",
  
  "manual_score_ai_collaboration": 12,
  "notes_ai_collaboration": "...",
  
  "custom_observations": "...",
  
  "final_score": 82,
  "evaluation_submitted_at": "2025-12-23T14:30:00Z"
}
```

---

## Evaluation Form UI Flow

### Section 1: Auto Score Review (2 minutes)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUTO SCORE REVIEW                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Test Results                           â”‚
â”‚  âœ“ 7/7 tests passed                    â”‚
â”‚  Auto Score: 87/100                    â”‚
â”‚                                         â”‚
â”‚  [ ] Manually adjust score              â”‚
â”‚      [87] / 100                         â”‚
â”‚      Why? [textarea]                    â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Section 2: Manual Assessment (10 minutes)
For each of 4 parameters:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â–¼ 1ï¸âƒ£ COMMUNICATION (0-15 points)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  ðŸ’¡ What to assess:                    â”‚
â”‚  Can they explain clearly? Correct     â”‚
â”‚  terminology? Trace examples?          â”‚
â”‚                                         â”‚
â”‚  Questions:                             â”‚
â”‚  â€¢ Walk me through your solution       â”‚
â”‚  â€¢ Explain this part in detail         â”‚
â”‚                                         â”‚
â”‚  ðŸš© Red Flags:                         â”‚
â”‚  âœ— Can't explain the code              â”‚
â”‚  âœ— Gets defensive                      â”‚
â”‚                                         â”‚
â”‚  âœ… Green Flags:                       â”‚
â”‚  âœ“ Clear step-by-step explanation     â”‚
â”‚  âœ“ Uses correct terminology            â”‚
â”‚                                         â”‚
â”‚  Score: [0â”€â”€â”€â”€â—â”€â”€â”€â”€15]                 â”‚
â”‚  Current: 11/15                        â”‚
â”‚                                         â”‚
â”‚  Notes (optional):                      â”‚
â”‚  [Good explanation but defensive       â”‚
â”‚   when challenged]                     â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Section 3: Custom Observations (2 minutes)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CUSTOM OBSERVATIONS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  [Large textarea]                       â”‚
â”‚                                         â”‚
â”‚  Growth signals, red flags, cultural    â”‚
â”‚  fit, role-specific notes...            â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Section 4: Final Score (Auto-Calculated)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FINAL SCORE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  Auto Score (40%):      87 Ã— 0.4 = 34.8â”‚
â”‚  Manual Score (60%):    78 Ã— 0.6 = 46.8â”‚
â”‚                                    â”€â”€â”€â”€ â”‚
â”‚  FINAL SCORE:                       82  â”‚
â”‚                                         â”‚
â”‚  Parameter Breakdown:                   â”‚
â”‚  â€¢ Communication: 11/15                â”‚
â”‚  â€¢ Algorithmic: 16/20                  â”‚
â”‚  â€¢ Problem Solving: 16/20              â”‚
â”‚  â€¢ AI Collaboration: 12/15             â”‚
â”‚                                         â”‚
â”‚  [Save Draft] [Submit Evaluation]      â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Backend Implementation

### ScoringService.java

```java
@Service
public class ScoringService {
    
    /**
     * Calculate final score from auto and manual scores
     */
    public int calculateFinalScore(int autoScore, InterviewSessionDTO manual) {
        double manualScore = calculateManualScore(manual);
        int finalScore = (int) Math.round(autoScore * 0.4 + manualScore * 0.6);
        return Math.min(100, Math.max(0, finalScore));
    }
    
    /**
     * Calculate manual score from 4 parameters
     * Formula: (comm/15 * 0.22 * 100) + (algo/20 * 0.28 * 100) + ...
     */
    public double calculateManualScore(InterviewSessionDTO manual) {
        double communication = (manual.communicationScore / 15.0) * 0.22 * 100;
        double algorithmic = (manual.algorithmicScore / 20.0) * 0.28 * 100;
        double problemSolving = (manual.problemSolvingScore / 20.0) * 0.28 * 100;
        double aiCollaboration = (manual.aiCollaborationScore / 15.0) * 0.22 * 100;
        
        return communication + algorithmic + problemSolving + aiCollaboration;
    }
    
    /**
     * Get auto score (either original or adjusted)
     */
    public int getAutoScore(InterviewSession session) {
        if (session.getAutoScoreAdjusted() != null) {
            return session.getAutoScoreAdjusted();
        }
        return session.getAutoScoreOriginal();
    }
}
```

### EvaluationController.java

```java
@RestController
@RequestMapping("/api/evaluations")
public class EvaluationController {
    
    @PostMapping
    public ResponseEntity<?> submitEvaluation(
        @RequestBody EvaluationRequest request,
        @AuthenticationPrincipal UserDetails user
    ) {
        // 1. Validate scores
        validateScores(request);
        
        // 2. Calculate final score
        int autoScore = request.getAutoScoreAdjusted() != null 
            ? request.getAutoScoreAdjusted() 
            : 87; // from tests
        
        double manualScore = scoringService.calculateManualScore(request);
        int finalScore = scoringService.calculateFinalScore(
            autoScore, 
            (int) manualScore
        );
        
        // 3. Save evaluation
        InterviewSession session = new InterviewSession();
        session.setAutoScoreAdjusted(request.getAutoScoreAdjusted());
        session.setAutoScoreAdjustedReason(request.getAutoScoreAdjustedReason());
        session.setManualScoreCommunication(request.getManualScoreCommunication());
        session.setManualScoreAlgorithmic(request.getManualScoreAlgorithmic());
        session.setManualScoreProblemSolving(request.getManualScoreProblemSolving());
        session.setManualScoreAiCollaboration(request.getManualScoreAiCollaboration());
        session.setNotesCommunication(request.getNotesCommunication());
        session.setNotesAlgorithmic(request.getNotesAlgorithmic());
        session.setNotesProblemSolving(request.getNotesProblemSolving());
        session.setNotesAiCollaboration(request.getNotesAiCollaboration());
        session.setCustomObservations(request.getCustomObservations());
        session.setFinalScore(finalScore);
        session.setEvaluationSubmittedAt(Instant.now());
        
        sessionRepository.save(session);
        
        return ResponseEntity.ok(new EvaluationResponse(session, finalScore));
    }
    
    @GetMapping("/{interviewId}")
    public ResponseEntity<?> getEvaluation(
        @PathVariable Long interviewId,
        @AuthenticationPrincipal UserDetails user
    ) {
        // Fetch and return evaluation
    }
    
    private void validateScores(EvaluationRequest request) {
        if (request.getManualScoreCommunication() < 0 || 
            request.getManualScoreCommunication() > 15) {
            throw new ValidationException("Communication score must be 0-15");
        }
        // ... validate others
    }
}
```

---

## Frontend React Component

See **EvaluationForm_Complete.jsx** for complete implementation.

Key features:
- Auto score review with optional adjustment
- 4 expandable parameter cards with sliders
- Red/green flags for each parameter
- Notes textarea for each parameter
- Custom observations textarea
- Auto-calculated final score display
- Submit button

---

## Scoring Examples

### Example 1: Strong Candidate

**Test Results:** 7/7 passed â†’ 87/100 auto score  
**Manual Assessment:**
- Communication: 14/15 (clear, responsive)
- Algorithmic: 18/20 (good edges, alternatives)
- Problem Solving: 17/20 (clean code, good debugging)
- AI Collaboration: 13/15 (tested, understood, improved)

**Calculation:**
```
Auto: 87 Ã— 0.4 = 34.8
Manual: ((14/15Ã—0.22) + (18/20Ã—0.28) + (17/20Ã—0.28) + (13/15Ã—0.22)) Ã— 100 Ã— 0.6
      = (20.93 + 25.2 + 23.8 + 19.07) Ã— 0.6
      = 88.99 Ã— 0.6
      = 53.4

Final: 34.8 + 53.4 = 88.2 â‰ˆ 88/100
```

**Interpretation:** Excellent candidate, ready to hire

---

### Example 2: Borderline Candidate

**Test Results:** 5/7 passed â†’ 71/100 auto score

**Manual Assessment:**
- Communication: 10/15 (decent but some confusion)
- Algorithmic: 14/20 (missed some edges)
- Problem Solving: 13/20 (code works but not clean)
- AI Collaboration: 9/15 (used AI, didn't fully understand)

**Calculation:**
```
Auto: 71 Ã— 0.4 = 28.4
Manual: ((10/15Ã—0.22) + (14/20Ã—0.28) + (13/20Ã—0.28) + (9/15Ã—0.22)) Ã— 100 Ã— 0.6
      = (14.67 + 19.6 + 18.2 + 13.2) Ã— 0.6
      = 65.67 Ã— 0.6
      = 39.4

Final: 28.4 + 39.4 = 67.8 â‰ˆ 68/100
```

**Interpretation:** Concerning patterns, needs significant improvement

---

### Example 3: Adjusted Auto Score

**Test Results:** 6/7 passed â†’ 78/100 auto score  
**Interviewer adjustment:** 85/100 (reason: "Failed edge case not in spec, solution excellent otherwise")

**Manual Assessment:**
- Communication: 12/15
- Algorithmic: 16/20
- Problem Solving: 15/20
- AI Collaboration: 11/15

**Calculation:**
```
Auto: 85 Ã— 0.4 = 34.0
Manual: ((12/15Ã—0.22) + (16/20Ã—0.28) + (15/20Ã—0.28) + (11/15Ã—0.22)) Ã— 100 Ã— 0.6
      = (17.6 + 22.4 + 21 + 16.13) Ã— 0.6
      = 77.13 Ã— 0.6
      = 46.3

Final: 34.0 + 46.3 = 80.3 â‰ˆ 80/100
```

**Interpretation:** Good baseline expectations

---

## Key Design Decisions

### Why 40/60 Split?
- **40% auto:** Objective measurement (tests pass/fail)
- **60% manual:** Your judgment (communication, thinking, approach)

Tests alone don't capture interviewer insights. Manual assessment captures collaboration quality and thinking process.

### Why These 4 Parameters?
- **Communication:** How well do they articulate ideas
- **Algorithmic Thinking:** How do they approach problems (edges, alternatives)
- **Problem Solving:** Code quality and debugging
- **AI Collaboration:** Your specific focusâ€”how effectively use AI as tool

### Why No Auto Recommendation?
- Different roles value different things
- Your team should make final decision
- Avoids algorithmic bias
- Keeps human judgment central

### Why Manual Adjustment for Auto Score?
- Tests can be too strict/lenient
- Context matters (environment issues, etc.)
- Gives flexibility for edge cases
- Requires documentation for audit trail

---

## Validation Rules

### Auto Score
- Range: 0-100
- Integer only
- If adjusted: require reason (max 500 chars)

### Manual Scores
- Communication: 0-15 integer
- Algorithmic: 0-20 integer
- Problem Solving: 0-20 integer
- AI Collaboration: 0-15 integer

### Notes
- Optional
- Max 1000 chars each
- Plain text

### Custom Observations
- Optional
- Max 2000 chars
- Plain text

### Final Score
- Auto-calculated
- Range: 0-100
- Integer only
- Can be stored but not manually edited

---

## Data Flow

```
1. Candidate submits solution
   â””â”€ Backend runs tests
   â””â”€ Auto score: 87/100

2. Interview ends
   â””â”€ Evaluation form loads
   â””â”€ Shows auto score: 87/100

3. Interviewer assesses
   â””â”€ Optional: adjust auto score
   â””â”€ Scores 4 parameters
   â””â”€ Adds notes for each
   â””â”€ Adds custom observations

4. Submit evaluation
   â””â”€ Backend calculates manual score: 78/100
   â””â”€ Calculates final: 82/100
   â””â”€ Saves all data
   â””â”€ Returns final score

5. Results displayed
   â””â”€ Final score: 82/100
   â””â”€ Score breakdown (40/60)
   â””â”€ Parameter details
   â””â”€ All notes visible
```

---

## Interviewer Workflow

**Total time: ~15 minutes**

| Step | Time | Action |
|------|------|--------|
| 1 | 2 min | Review auto score (adjust if needed) |
| 2 | 2 min | Assess Communication parameter |
| 3 | 2 min | Assess Algorithmic Thinking parameter |
| 4 | 2 min | Assess Problem Solving parameter |
| 5 | 2 min | Assess AI Collaboration parameter |
| 6 | 2 min | Add custom observations |
| 7 | 1 min | Review auto-calculated final score |
| 8 | 1 min | Submit evaluation |

---

## Reports & Analytics (Future)

While not in MVP, think about:
- Average score by parameter
- Score distribution (how many 80+, 90+, etc)
- Parameters with highest variance
- Interviewer calibration (do different interviewers score differently?)
- Question difficulty (which questions get lower scores)

---

## Security Considerations

- Only interviewer who conducted interview can submit evaluation
- Evaluation data immutable after submission
- Audit trail of any manual adjustments
- No interviewer can see other interviewers' scores (unless admin)
- Final score visible to hiring team
- All notes visible to hiring team

---

## Summary

Your scoring system is:
- **Transparent:** Weights and formulas clear
- **Flexible:** Can adjust auto score with reason
- **Guided:** Parameter definitions with examples
- **Focused:** Your 4 custom parameters
- **Fair:** Mix of objective (tests) and subjective (judgment)
- **Documented:** Full audit trail of reasoning

This gives you **objective data** (tests) + **human judgment** (your assessments) = **fair, defensible hiring decisions**.

---

*Scoring System Version: Final*  
*Status: Ready to Implement*  
*Last Updated: December 2025*
